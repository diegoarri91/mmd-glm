{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We first load the spiking data as a tensor `mask_spikes_d` with {0, 1} values and we define a 1d tensor `t` with the time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmdglm.utils import plot_spiketrain\n",
    "from examples_utils import fig_layout, plot_fit, psth_and_autocor, set_style\n",
    "\n",
    "set_style()\n",
    "\n",
    "mask_spikes_d = np.load('./monkey_pmv.npy')\n",
    "mask_spikes_d = torch.from_numpy(mask_spikes_d).float()\n",
    "\n",
    "dt = 1\n",
    "t = torch.arange(0, len(mask_spikes_d), 1) * dt\n",
    "\n",
    "ax = plot_spiketrain(t, mask_spikes_d, label='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute quantities that we will use to assess goodness-of-fit of the models we will fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mmdglm.convkernels.values import KernelBasisValues\n",
    "from kernel.fun import KernelFun\n",
    "from mmdglm.metrics import bernoulli_log_likelihood_pp\n",
    "\n",
    "n_spk = torch.sum(mask_spikes_d) # total number of spikes\n",
    "ll_pp = bernoulli_log_likelihood_pp(mask_spikes_d) # bernoulli likelihood of poisson process with the same rate\n",
    "\n",
    "kernel_smooth = KernelFun.gaussian(sigma=torch.tensor([20]), \n",
    "                                   weight=torch.tensor([1.]), \n",
    "                                   requires_grad=False)\n",
    "# kernel_smooth = KernelBasisValues.gaussian(dt, tau=20) # gaussian kernel for smoothing spike trains\n",
    "arg_last_lag = 200 # last lag used in autocorrelation\n",
    "t_autocor = torch.arange(0, arg_last_lag, 1) # autocorrelation time points\n",
    "    \n",
    "psth_d, autocor_d = psth_and_autocor(t, mask_spikes_d, kernel_smooth=kernel_smooth, \n",
    "                                     smooth_autocor=False, arg_last_lag=arg_last_lag) # compute PSTH and autocorrelation of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit ml-glm\n",
    "\n",
    "We first fit a GLM by maximum likelihood. \n",
    "\n",
    "We load the basis values for the history filter and define a filter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = np.load('basis.npy')\n",
    "hist = KernelBasisValues(dt=dt, basis_values=basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a GLM instance and extract its parameters maximizing the likelihood using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from mmdglm.glm.torch import TorchGLM\n",
    "\n",
    "mlglm = TorchGLM(u0=-5, eta=hist)\n",
    "optim = Adam(mlglm.parameters(), lr=1e-1)\n",
    "loss, _ = mlglm.train(t, mask_spikes_d, num_epochs=200, optim=optim, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the log-likelihood of the ml-glm, generate samples and compute the psth and autocorrelation of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ml = (-loss[-1] - ll_pp) / np.log(2) / n_spk # log-likelihood per spike relative to a poisson process with the same rate \n",
    "\n",
    "t_long = torch.arange(0, 3000, dt)\n",
    "_, _, mask_spikes_ml = mlglm.sample(t_long, shape=(500,)) # generate 500 samples from ml-glm\n",
    "\n",
    "psth_ml, autocor_ml = psth_and_autocor(t_long, mask_spikes_ml, kernel_smooth=kernel_smooth, last_lag=last_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ml-glm produces free-running samples with diverging firing rate due to the history filter's self excitation. The free-running samples also fail to capture the autocorrelation of the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = fig_layout()\n",
    "plot_fit(axs, label='data', mask_spikes=mask_spikes_d, psth=psth_d, autocor=autocor_d)\n",
    "plot_fit(axs, label='ml-glm', mask_spikes=mask_spikes_ml[:, :10], psth=psth_ml, history_filter=mlglm.eta, autocor=autocor_ml, ll=ll_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MMD-GLM\n",
    "\n",
    "To improve the free-running behavior of the GLM we propose to optimize a kernel-induced statistic called Maximum Mean Discrepancy (MMD). Given a kernel $k$ or its associated feature map $\\phi: X \\rightarrow \\mathscr{H}$, with $k(X, X')=\\langle \\phi(X), \\phi(X') \\rangle_{\\mathscr{H}}$ and $\\mathscr{H}$ the reproducing kernel Hilbert space, MMD measures the distance in $\\mathscr{H}$ between the embeddings of two probability distributions $\\hat{p}$ and $p$  \n",
    "\n",
    "\\begin{equation*}\n",
    "d_\\text{MMD}(\\hat{p}, p)^2 = \\left| \\left| E_{X \\sim \\hat{p}}[\\phi(X)] - E_{X' \\sim p}[\\phi(X')] \\right| \\right|_\\mathscr{H}^2\n",
    "\\end{equation*}\n",
    "\n",
    "In this example we minimize the objective function $\\text{NLL} + \\alpha \\text{MMD}^2$, with $\\text{NLL}$ the negative log-likelihood and $\\alpha$ a hyperparameter that controls the relative weight of $\\text{MMD}$. We use a model-based feature map, that is a feature map extracted from the GLM we are trying to fit. The feature map\n",
    "\\begin{equation*}\n",
    "\\phi(x; \\theta) = \\sum_{t=1}^\\tau H_x(t; h) H_x(t + \\tau; h) \\hspace{1cm} H_x(t; h)=\\sum_{\\tau} h_\\tau x_{t - \\tau}\n",
    "\\end{equation*}\n",
    "is the autocorrelation of $H_x$, the convolution of the history filter $h$ with the spike train $x$.\n",
    "\n",
    "To minimize the induced MMD, we first define the function that computes this feature map from the conditional intensity $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def phi_autocor_history(t, lam, model, last_lag=500):\n",
    "    \"\"\"\n",
    "    Returns the autocorrelation of the history term.\n",
    "    Functions to compute model-based feature maps should be of the form phi(t, lam, model, **kwargs)\n",
    "    \"\"\"\n",
    "    T = len(t)\n",
    "    history = torch.log(lam) - model.b\n",
    "    autocov = F.conv1d(history.T[None, :, :], history.T[:, None, :], padding=last_lag, groups=history.shape[1]) / T\n",
    "    autocov = autocov[0, :, last_lag + 1:].T # get only positive lags\n",
    "    return autocov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `ModelBasedMMDGLM` class to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdglm.glm.modelbasedmmd import ModelBasedMMDGLM\n",
    "\n",
    "alpha_mmd = 1.5e3 # MMD relative weight against NLL in the cost function\n",
    "n_batch_fr = 50 # Number of free-running samples drawn at each iteration to compute MMD^2 and its gradient \n",
    "num_epochs = 300\n",
    "\n",
    "mmdglm = ModelBasedMMDGLM(u0=mlglm.u0, eta=mlglm.eta.copy()) # We initialize the optimization the MLE values\n",
    "optim = Adam(mmdglm.parameters(), lr=1e-2)\n",
    "\n",
    "loss_mmd, metrics = mmdglm.train(t, mask_spikes_d, phi=phi_autocor_history, alpha_mmd=alpha_mmd, \n",
    "                                 n_batch_fr=n_batch_fr, num_epochs=num_epochs, optim=optim, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMD$^2$ decreases during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['mmd'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel(r'MMD$^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the log-likelihood of the mmd-glm, generate samples and compute the psth and autocorrelation of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_mmd = (-metrics['nll'][-1] - ll_pp) / np.log(2) / n_spk\n",
    "_, _, mask_spikes_mmd = mmdglm.sample(t_long, shape=(500,))\n",
    "psth_mmd, autocor_mmd = psth_and_autocor(t_long, mask_spikes_mmd, kernel_smooth=kernel_smooth, last_lag=last_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mmd-glm produces free-running samples with stable firing rate by preferentially reducing the history filter's late self excitation. The free-running samples better match the autocorrelation of the data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = fig_layout(mmd=True)\n",
    "plot_fit(axs, label='data', mask_spikes=mask_spikes_d, psth=psth_d, autocor=autocor_d)\n",
    "plot_fit(axs, label='ml-glm', mask_spikes=mask_spikes_ml[:, :10], psth=psth_ml, history_filter=mlglm.eta, autocor=autocor_ml, ll=ll_ml)\n",
    "plot_fit(axs, label='mmd-glm', mask_spikes=mask_spikes_mmd[:, :10], psth=psth_mmd, history_filter=mmdglm.eta, autocor=autocor_mmd, ll=ll_mmd)\n",
    "axs[2].legend(frameon=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdglm",
   "language": "python",
   "name": "mmdglm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
