import numpy as np
from scipy.stats import kstest
import torch


def mmd_loss(t, s1, s2, phi=None, kernel=None, biased=False, **kwargs):
    r"""Computes MMD between s1 and s2 using a feature map or a kernel. 
    Doesn't support compute gradients"""
    with torch.no_grad():

        n1, n2 = s1.shape[1], s2.shape[1]
        
        if kernel is not None:
            gramian_11 = kernel(t, s1, s1, **kwargs)
            gramian_22 = kernel(t, s2, s2, **kwargs)
            gramian_12 = kernel(t, s1, s2, **kwargs)
            if not biased:
                gramian_11.fill_diagonal_(0)
                gramian_22.fill_diagonal_(0)
                mmd = torch.sum(gramian_11) / (n1 * (n1 - 1)) + torch.sum(gramian_22) / (n2 * (n2 - 1)) \
                                              - 2 * torch.mean(gramian_12)
            else:
                mmd = torch.mean(gramian_11) + torch.mean(gramian_22) - 2 * torch.mean(gramian_12)

        elif phi is not None:
            phi_1 = phi(t, s1, **kwargs)
            phi_2 = phi(t, s2, **kwargs)
            if biased:
                phi_1_mean = torch.mean(phi_1, 1)
                phi_2_mean = torch.mean(phi_2, 1)
                mmd = torch.sum((phi_1_mean - phi_2_mean)**2)
            else:
                sum_phi_1 = torch.sum(phi_1, 1)
                sum_phi_2 = torch.sum(phi_2, 1)
                norm2_1 = (torch.sum(sum_phi_1**2) - torch.sum(phi_1**2)) / (n1 * (n1 - 1))
                norm2_2 = (torch.sum(sum_phi_2**2) - torch.sum(phi_2**2)) / (n2 * (n2 - 1))
                mean_dot = torch.sum(sum_phi_1 * sum_phi_2) / (n1 * n2)     
                mmd = norm2_1 + norm2_2 - 2 * mean_dot
        
    return mmd


def negative_log_likelihood(dt, mask_spikes, log_lam, reduction='sum'):
    r"""Computes the negative log-likelihood of the data"""
    mask_spikes = mask_spikes.type(log_lam.dtype)
    # log_like = torch.sum(log_lam * mask_spikes, dim=0) - dt * torch.sum(log_lam.exp() * (1 - mask_spikes), dim=0)
    lam = log_lam.exp()
    p_spk = 1 - torch.exp(-lam * dt) + 1e-24
    log_like = torch.sum(p_spk.log() * mask_spikes, dim=0) - dt * torch.sum(lam * (1 - mask_spikes), dim=0)
    if reduction == 'sum':
        log_like = log_like.sum()
    elif reduction == 'mean':
        log_like = log_like.mean()
    elif reduction == 'none':
        pass
    else:
        raise RuntimeError("Value {:s} for reduction is not valid. Valid options are 'none', 'mean' and 'sum'.")
    return -log_like


def negative_log_likelihood_pp(mask_spikes):
    r"""Computes the Bernoulli log-likelihood of the samples assuming they were generated by a Poisson process"""
    n_spk = torch.sum(mask_spikes)
    n_nospk = mask_spikes.numel() - n_spk
    p_spk = n_spk / mask_spikes.numel()
    log_like_pp = n_spk * torch.log(p_spk) + n_nospk * torch.log(1 - p_spk)
    return -log_like_pp


# def poisson_log_likelihood_poisson_process(dt, mask_spikes, u, r):
#     r"""Computes the Poisson log-likelihood of the samples relative to the log-likelihood
#     of a poisson process with the same mean rate"""
#     lent = mask_spikes.shape[0]
#     n_spikes = np.sum(mask_spikes, 0)
#     n_spikes = n_spikes[n_spikes > 0]
#     log_likelihood = np.sum(u[mask_spikes]) - dt * np.sum(r) + np.sum(n_spikes) * np.log(dt)
#     log_likelihood_poisson = np.sum(n_spikes * (np.log(n_spikes / lent) - 1))
#     log_like_normed = (log_likelihood - log_likelihood_poisson) / np.log(2) / np.sum(n_spikes)
#     return log_like_normed


def time_rescale_transform(dt, mask_spikes, r):
    r"""Computes the time rescale transform of the data given the predicted rate
    and the results of the KS test when comparing to the uniform distribution"""
    integral_r = np.cumsum(r * dt, axis=0)

    z = []
    for sw in range(mask_spikes.shape[1]):
        integral_r_spikes = integral_r[mask_spikes[:, sw], sw] 
        z += [1. - np.exp(-(integral_r_spikes[1:] - integral_r_spikes[:-1]))]

    ks_stats = kstest(np.concatenate(z), 'uniform', args=(0, 1))

    return z, ks_stats


def _append_metrics(metrics_list, _metrics):
    r"""Appends the metrics to the metrics dictionary"""
    if metrics_list is None:
        metrics_list = {key:[val] for key, val in _metrics.items()}
    else:
        for key, val in _metrics.items():
            metrics_list[key].append(val)
    return metrics_list


def _mmd_from_features(t, phi_1, phi_2, biased=False):
    r"""Computes MMD from the feature maps"""
    n1, n2 = phi_1.shape[1], phi_2.shape[1]
    if biased:
        phi_1_mean = torch.mean(phi_1, 1)
        phi_2_mean = torch.mean(phi_2, 1)
        mmd = torch.sum((phi_1_mean - phi_2_mean)**2)
    else:
        sum_phi_1 = torch.sum(phi_1, 1)
        sum_phi_2 = torch.sum(phi_2, 1)
        norm2_1 = (torch.sum(sum_phi_1**2) - torch.sum(phi_1**2)) / (n1 * (n1 - 1))
        norm2_2 = (torch.sum(sum_phi_2**2) - torch.sum(phi_2**2)) / (n2 * (n2 - 1))
        mean_dot = torch.sum(sum_phi_1 * sum_phi_2) / (n1 * n2)
        mmd = norm2_1 + norm2_2 - 2 * mean_dot
    return mmd


def _mmd_surrogate_from_features(t, phi_d, phi_fr, log_prob, biased=False):
    log_prob_phi_fr = log_prob[None, :] * phi_fr
    if biased:
        phi_d_mean = torch.mean(phi_d, 1)
        phi_fr_mean = torch.mean(phi_fr, 1)
        mmd_surr = -2 * torch.sum(log_prob_phi_fr.mean(dim=1) * (phi_d_mean - phi_fr_mean))
    else:
        n_d, n_fr = phi_d.shape[1], phi_fr.shape[1]
        sum_phi_d = torch.sum(phi_d, 1)
        sum_phi_fr = torch.sum(phi_fr, 1)
        sum_log_proba_phi_fr = torch.sum(log_prob_phi_fr, 1)
        norm2_fr = (torch.sum(sum_log_proba_phi_fr * sum_phi_fr) - torch.sum(log_prob_phi_fr * phi_fr)) / (
                    n_fr * (n_fr - 1))
        mmd_surr = 2 * norm2_fr - 2 / (n_d * n_fr) * torch.sum(sum_phi_d * sum_log_proba_phi_fr)
    return mmd_surr


def _mmd_from_gramians(t, gramian_11, gramian_22, gramian_12, biased=False):
    r"""Computes MMD from the gramian matrices"""
    n1, n2 = gramian_11.shape[0], gramian_22.shape[0]
    if not biased:
        gramian_11.fill_diagonal_(0)
        gramian_22.fill_diagonal_(0)
        mmd = torch.sum(gramian_11) / (n1 * (n1 - 1)) + torch.sum(gramian_22) / (n2 * (n2 - 1)) \
                                              - 2 * torch.mean(gramian_12)
    else:
        mmd = torch.mean(gramian_11) + torch.mean(gramian_22) - 2 * torch.mean(gramian_12)
    return mmd


def _mmd_surrogate_from_gramians(t, gramian_fr_fr, gramian_d_fr, log_prob, biased=False):

    if biased:
        mmd_surr = torch.mean(((log_prob[:, None] + log_prob[None, :]) * gramian_fr_fr)) \
                   - 2 * torch.mean(log_prob[None, :] * gramian_d_fr)
    else:
        n_fr = gramian_fr_fr.shape[0]
        n_off_diag = n_fr * (n_fr - 1)
        gramian_fr_fr.fill_diagonal_(0)
        mmd_surr = (log_prob[:, None] * gramian_fr_fr).sum() / n_off_diag - (log_prob[None, :] * gramian_d_fr).mean()
        mmd_surr = 2 * mmd_surr
    return mmd_surr
